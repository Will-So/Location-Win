{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove the food items from the lda as well\n",
    "- Describe more what I do in this \n",
    "- Do LDA by poorly related restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I use LDA to model latent topics. Latent models don't always yield useful results and it is difficult to know whether they will be useful or not until we actually start. \n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Group the Data by restauarant and then squish the review text together. \n",
    "\n",
    "Extensions:\n",
    "\n",
    "1. Consider only positive reviews. (Happy in the same say, upset for all different ways)\n",
    "2. COnsider only negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.ldamulticore import LdaModel\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_hdf('../data/restaurant_reviews.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'review_id', 'text', 'user_id', 'city', 'latitude', 'longitude',\n",
       "       'name', 'neighborhoods', 'stars', 'hours'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_rest = combined_data.groupby(combined_data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15321"
      ]
     },
     "execution_count": 6,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(g_rest.stars.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = g_rest.text.apply(lambda x: '\\n'.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    So this place isn't spectacular, but it is def...\n",
       "1    I really like this place. The portions are lar...\n",
       "2    Delicious Sushi! Very big, fresh, tasty, flavo...\n",
       "3    Pizza was just plain, nothing special. I have ...\n",
       "4    I don't like the pizza hear and if you go for ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "text.text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.text = text.text.apply(lambda x: gensim.utils.simple_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [so, this, place, isn, spectacular, but, it, i...\n",
       "1    [really, like, this, place, the, portions, are...\n",
       "2    [delicious, sushi, very, big, fresh, tasty, fl...\n",
       "3    [pizza, was, just, plain, nothing, special, ha...\n",
       "4    [don, like, the, pizza, hear, and, if, you, go...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "text.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import os\n",
    "# data_dir = '/Users/Will/anaconda/envs/py34/lib/python3.4/site-packages/spacy/en/data'\n",
    "# nlp = spacy.en.English(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make a gensim dictionary that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242663"
      ]
     },
     "execution_count": 12,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the Corpus into the format we want and save it to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start out with, we use the nltk stopwords. If this turns out to cause problems, we will also start excluding the most common words from the current corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ids = list(map(dictionary.token2id.get, stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 16,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(stopwords_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_tokens(stopwords_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41555"
      ]
     },
     "execution_count": 19,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(t) for t in text.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.corpora.MmCorpus.serialize('../data/deerwester.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('../data/words.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should probably remove stopwords at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus \n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load('../data/words.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = MmCorpus('../data/deerwester.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend running this on a power EC2 server. Or else it will take too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lda = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=20, workers=31,\n",
    "                       chunksize=50, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lda.save('../data/ldamodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=242663, num_topics=20, decay=0.5, chunksize=100)\n"
     ]
    }
   ],
   "source": [
    "print(all_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.050*the + 0.033*and + 0.027*to + 0.019*it + 0.016*of + 0.014*was + 0.014*is + 0.013*for + 0.013*in + 0.011*this',\n",
       " '0.054*the + 0.037*and + 0.023*to + 0.019*pizza + 0.019*it + 0.019*was + 0.016*of + 0.015*is + 0.013*for + 0.012*in',\n",
       " '0.056*the + 0.039*and + 0.023*to + 0.019*was + 0.018*it + 0.017*is + 0.016*of + 0.013*for + 0.011*in + 0.010*food',\n",
       " '0.056*the + 0.032*and + 0.024*to + 0.020*was + 0.019*of + 0.017*it + 0.014*for + 0.013*is + 0.012*in + 0.011*buffet',\n",
       " '0.053*the + 0.036*and + 0.026*to + 0.017*of + 0.016*it + 0.016*was + 0.014*is + 0.012*for + 0.011*in + 0.010*we',\n",
       " '0.055*the + 0.037*and + 0.022*was + 0.022*to + 0.019*it + 0.015*of + 0.013*for + 0.012*we + 0.011*is + 0.010*in',\n",
       " '0.057*the + 0.037*and + 0.022*to + 0.019*it + 0.018*was + 0.017*of + 0.015*is + 0.013*in + 0.011*for + 0.010*pho',\n",
       " '0.056*the + 0.035*and + 0.023*to + 0.018*it + 0.018*was + 0.016*burger + 0.014*of + 0.012*is + 0.012*in + 0.011*for',\n",
       " '0.053*the + 0.035*and + 0.022*to + 0.019*it + 0.019*was + 0.016*is + 0.015*of + 0.012*in + 0.012*for + 0.012*food',\n",
       " '0.056*the + 0.038*and + 0.021*to + 0.018*was + 0.018*it + 0.015*of + 0.015*is + 0.012*in + 0.011*for + 0.010*food']"
      ]
     },
     "execution_count": 35,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "all_lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_single = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20,\n",
    "                     chunksize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.050*pad + 0.032*pittsburgh + 0.023*tom + 0.011*truck + 0.009*panang + 0.009*drunken + 0.007*ew + 0.006*satay + 0.006*papaya + 0.005*curries',\n",
       " '0.006*pepperoni + 0.004*subway + 0.004*zpizza + 0.004*philly + 0.003*factory + 0.003*refund + 0.003*pies + 0.002*hut + 0.002*cheesesteak + 0.002*wich',\n",
       " '0.021*brisket + 0.017*spaghetti + 0.012*ravioli + 0.010*lasagna + 0.009*ricotta + 0.009*bruschetta + 0.008*gelato + 0.008*marinara + 0.008*tiramisu + 0.007*pastas',\n",
       " '0.307*pho + 0.068*vietnamese + 0.033*unphogettable + 0.013*mi + 0.011*boba + 0.010*vermicelli + 0.010*sprouts + 0.010*brisket + 0.009*bo + 0.008*plaza',\n",
       " '0.013*charlotte + 0.006*grits + 0.004*cornbread + 0.004*tots + 0.003*vig + 0.003*uptown + 0.003*ahi + 0.003*soul + 0.003*kale + 0.002*lemonade',\n",
       " '0.034*pita + 0.031*hummus + 0.027*gyro + 0.018*falafel + 0.014*edinburgh + 0.013*feta + 0.012*mediterranean + 0.010*gyros + 0.007*kabob + 0.007*eastern',\n",
       " '0.019*pub + 0.013*madison + 0.010*shake + 0.008*pickles + 0.007*pretzel + 0.007*irish + 0.006*craft + 0.006*buns + 0.006*sliders + 0.006*curds',\n",
       " '0.045*asada + 0.043*carne + 0.024*burritos + 0.017*pastor + 0.015*tortillas + 0.015*torta + 0.012*el + 0.010*quesadilla + 0.009*pico + 0.009*viva',\n",
       " '0.017*noca + 0.013*foie + 0.008*truffle + 0.008*gras + 0.006*pairing + 0.005*risotto + 0.004*candy + 0.004*pairings + 0.004*mussels + 0.004*amuse',\n",
       " '0.005*yard + 0.004*wing + 0.004*band + 0.003*dance + 0.003*mill + 0.003*zipps + 0.003*upstairs + 0.002*ticket + 0.002*entertainment + 0.002*smoking']"
      ]
     },
     "execution_count": 97,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "lda_single.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_single.save('../data/all_lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Reviews Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(df, grouper='name', text='text', save=False):\n",
    "    \"\"\"\n",
    "    Generates a dictionary with stopwords and most frequent words removed.\n",
    "    Also tells how many items have been removed. Optionally allows saving the\n",
    "    files to disk.\n",
    "\n",
    "\n",
    "    :param df: pd.DataFrame()\n",
    "    :param grouper: The column that we want to group by. For example, by restaurant.\n",
    "    :param text: The column with the text that we want to model by\n",
    "    :return: corpus, and dict object.\n",
    "\n",
    "    Example\n",
    "    ----\n",
    "    >>> corpus, dictionary = generate_corpus(df, save=True) # doctest: +SKIP\n",
    "    \"\"\"\n",
    "    # Combine the text on a restaurant level\n",
    "    grouped_df = df.groupby(grouper)\n",
    "    all_review_text = grouped_df[text].apply(lambda x: '\\n'.join(x)).reset_index()\n",
    "\n",
    "    # Do some Processing\n",
    "    all_review_text[text] = all_review_text[text].apply(\n",
    "        lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "    # Generate the dictionary\n",
    "    dictionary = generate_dictionary(all_review_text[text])\n",
    "\n",
    "    # Generate the corpus\n",
    "    corpus = [dictionary.doc2bow(t) for t in all_review_text[text]]\n",
    "\n",
    "    if save:\n",
    "        try:\n",
    "            gensim.corpora.MmCorpus.serialize('../data/corpus', corpus)\n",
    "            dictionary.save('../data/words.dict')\n",
    "        except IOError as e:\n",
    "            logging.warning(\"Could not save to disk. Exception {}\".format(e))\n",
    "\n",
    "    return corpus, dictionary\n",
    "\n",
    "\n",
    "def generate_dictionary(series):\n",
    "    \"\"\"\n",
    "    Generates a gensim dictionary. \n",
    "\n",
    "    :param series:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dictionary = gensim.corpora.Dictionary(series)\n",
    "    original_dict_length = len(dictionary)\n",
    "\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    stopwords_ids = list(map(dictionary.token2id.get, stopwords))\n",
    "    dictionary.filter_tokens(stopwords_ids)\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.1)\n",
    "\n",
    "    dictionary.compactify()\n",
    "    percent_removed = 1 - (len(dictionary) / original_dict_length)\n",
    "    logging.info('Removed {} items from our dictionary. {} %'\n",
    "                 .format(original_dict_length - len(dictionary),\n",
    "                         percent_removed))\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews = combined_data[combined_data.stars.isin([4,5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_corpus, good_dict = generate_corpus(good_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "corpora.MmCorpus.serialize('../data/good_corpus.mm', good_corpus)\n",
    "good_dict.save('../data/good_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_lda = LdaModel(corpus=good_corpus, id2word=good_dict, num_topics=20,\n",
    "                     chunksize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33514"
      ]
     },
     "execution_count": 125,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(good_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.033*pub + 0.026*deli + 0.016*pastrami + 0.015*bagel + 0.012*irish + 0.011*pretzel + 0.011*craft + 0.010*bagels + 0.010*subway + 0.009*ale',\n",
       " '0.063*pancakes + 0.037*waffle + 0.031*waffles + 0.031*benedict + 0.030*hash + 0.022*omelet + 0.017*pancake + 0.016*biscuits + 0.015*omelette + 0.014*syrup',\n",
       " '0.042*japanese + 0.031*sashimi + 0.023*ramen + 0.023*ayce + 0.022*tempura + 0.021*miso + 0.018*sake + 0.016*nigiri + 0.015*teriyaki + 0.013*yellowtail',\n",
       " '0.026*asada + 0.025*carne + 0.025*guacamole + 0.021*margaritas + 0.017*margarita + 0.015*chile + 0.015*tortillas + 0.014*enchiladas + 0.014*chipotle + 0.013*burritos',\n",
       " '0.135*pho + 0.067*indian + 0.044*vietnamese + 0.022*naan + 0.014*mi + 0.014*masala + 0.009*banh + 0.009*tikka + 0.008*yasu + 0.008*paneer',\n",
       " '0.037*le + 0.036*et + 0.033*montreal + 0.027*est + 0.019*les + 0.015*en + 0.014*une + 0.014*que + 0.014*des + 0.013*pas',\n",
       " '0.048*und + 0.031*der + 0.028*das + 0.027*ist + 0.022*sehr + 0.021*es + 0.019*ich + 0.018*auch + 0.017*airport + 0.015*zu',\n",
       " '0.119*brisket + 0.045*yama + 0.045*texas + 0.043*ayce + 0.030*slaw + 0.028*smoke + 0.026*barbecue + 0.021*coleslaw + 0.018*pig + 0.017*cole',\n",
       " '0.008*foie + 0.007*risotto + 0.007*gras + 0.006*courses + 0.006*truffle + 0.006*mussels + 0.005*goat + 0.004*marrow + 0.004*noca + 0.004*bass',\n",
       " '0.017*filet + 0.011*steaks + 0.009*truffle + 0.008*oz + 0.008*steakhouse + 0.007*ahi + 0.007*ribeye + 0.007*bisque + 0.007*mignon + 0.006*asparagus']"
      ]
     },
     "execution_count": 134,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "good_lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_lda.save('../data/good_lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Reviews Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_reviews = combined_data[combined_data.stars.isin([1,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_corpus, bad_dict = generate_corpus(bad_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_lda = LdaModel(corpus=bad_corpus, id2word=bad_dict, num_topics=20,\n",
    "                   chunksize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.005*airport + 0.004*pittsburgh + 0.003*flight + 0.003*fox + 0.002*presentation + 0.002*bay + 0.002*tuna + 0.002*wolfgang + 0.002*overrated + 0.002*puck',\n",
       " '0.023*chocolate + 0.022*dog + 0.015*toast + 0.015*cafe + 0.013*brunch + 0.013*waffle + 0.013*turkey + 0.011*dogs + 0.008*shop + 0.008*smoothie',\n",
       " '0.200*sushi + 0.023*tuna + 0.020*japanese + 0.016*sashimi + 0.016*tempura + 0.015*ayce + 0.012*teriyaki + 0.012*nigiri + 0.011*miso + 0.008*chefs',\n",
       " '0.017*burgers + 0.011*beers + 0.007*sports + 0.005*pub + 0.005*bun + 0.005*bartenders + 0.005*rings + 0.004*sliders + 0.004*tv + 0.004*nachos',\n",
       " '0.058*thai + 0.021*curry + 0.018*pad + 0.010*spaghetti + 0.009*basil + 0.009*tom + 0.008*olive + 0.008*meatballs + 0.006*garden + 0.006*lasagna',\n",
       " '0.041*lobster + 0.013*scallops + 0.011*filet + 0.010*lamb + 0.009*oysters + 0.009*risotto + 0.009*duck + 0.008*chocolate + 0.008*steaks + 0.007*calamari',\n",
       " '0.063*ramen + 0.056*korean + 0.031*tapas + 0.017*broth + 0.014*boba + 0.013*tofu + 0.010*belly + 0.009*shabu + 0.008*kimchi + 0.007*truck',\n",
       " '0.005*thru + 0.004*window + 0.004*cashier + 0.003*wendy + 0.003*subway + 0.003*register + 0.003*workers + 0.003*receipt + 0.002*locations + 0.002*machine',\n",
       " '0.081*tacos + 0.037*burrito + 0.018*guacamole + 0.018*tortilla + 0.016*carne + 0.016*asada + 0.015*margarita + 0.013*tortillas + 0.012*bell + 0.011*margaritas',\n",
       " '0.024*ribs + 0.020*mac + 0.012*brisket + 0.007*meats + 0.007*smoked + 0.007*smoke + 0.006*slaw + 0.006*rib + 0.006*texas + 0.006*mashed']"
      ]
     },
     "execution_count": 133,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "bad_lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('../data/bad_corpus.mm', bad_corpus)\n",
    "bad_dict.save('../data/bad_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_lda.save('../data/bad_lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhoods</th>\n",
       "      <th>stars</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>6w6gMZ3iBLGcUM4RBIuifQ</td>\n",
       "      <td>This place was DELICIOUS!!  My parents saw a r...</td>\n",
       "      <td>LWbYpcangjBMm4KPxZGOKg</td>\n",
       "      <td>Braddock</td>\n",
       "      <td>40.408735</td>\n",
       "      <td>-79.866351</td>\n",
       "      <td>Emil's Lounge</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>{'Tuesday': {'close': '19:00', 'open': '10:00'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-03-15</td>\n",
       "      <td>jVVv_DA5mCDB6mediuwHAw</td>\n",
       "      <td>Can't miss stop for the best Fish Sandwich in ...</td>\n",
       "      <td>m1FpV3EAeggaAdfPx0hBRQ</td>\n",
       "      <td>Braddock</td>\n",
       "      <td>40.408735</td>\n",
       "      <td>-79.866351</td>\n",
       "      <td>Emil's Lounge</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>{'Tuesday': {'close': '19:00', 'open': '10:00'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-03-30</td>\n",
       "      <td>3Es8GsjkssusYgeU6_ZVpQ</td>\n",
       "      <td>This place should have a lot more reviews - bu...</td>\n",
       "      <td>8fApIAMHn2MZJFUiCQto5Q</td>\n",
       "      <td>Braddock</td>\n",
       "      <td>40.408735</td>\n",
       "      <td>-79.866351</td>\n",
       "      <td>Emil's Lounge</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>{'Tuesday': {'close': '19:00', 'open': '10:00'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-10-20</td>\n",
       "      <td>KAkcn7oQP1xX8KsZ-XmktA</td>\n",
       "      <td>This place was very good. I found out about Em...</td>\n",
       "      <td>uK8tzraOp4M5u3uYrqIBXg</td>\n",
       "      <td>Braddock</td>\n",
       "      <td>40.408735</td>\n",
       "      <td>-79.866351</td>\n",
       "      <td>Emil's Lounge</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>{'Tuesday': {'close': '19:00', 'open': '10:00'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>BZNJkkP0bXnwQ2-sCqat2Q</td>\n",
       "      <td>Old school.....traditional \"mom 'n pop\" qualit...</td>\n",
       "      <td>6wvlM5L4_EroGXbnb_92xQ</td>\n",
       "      <td>Braddock</td>\n",
       "      <td>40.408735</td>\n",
       "      <td>-79.866351</td>\n",
       "      <td>Emil's Lounge</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>{'Tuesday': {'close': '19:00', 'open': '10:00'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Traits of Poorly Rated Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15321"
      ]
     },
     "execution_count": 27,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(g_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rest_ratings = g_rest.stars.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15321"
      ]
     },
     "execution_count": 7,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(avg_rest_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_reviewed = combined_data[combined_data.name.isin(well_reviewed.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#1 Brothers Pizza', '#1 Sushi', '1 Brothers Pizza',\n",
       "       '10-to-10 In Delhi', '1000 Grammes', '108 Chinese Take Away',\n",
       "       '11 Scalini', '131 Main', '188 Restaurant',\n",
       "       '1923 Bourbon & Burlesque By Holly Madison',\n",
       "       ...\n",
       "       'noca', 'notion', 'parker's PGH', 's' Wölfle Bistro', 's.e.e.d. cafe',\n",
       "       'the Goodwich', 'unPhogettable', 'z Fondue', 'Özge Imbiss',\n",
       "       'é by José Andrés'],\n",
       "      dtype='object', name='name', length=7385)"
      ]
     },
     "execution_count": 43,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "well_reviewed.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorly_reviewed = avg_rest_ratings[avg_rest_ratings < 3.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#1 Hawaiian Barbecue', '#1Brothers Pizza', '1 Brother's Pizza',\n",
       "       '1 Eastern Super Buffet', '100% Natural Mexican Grill',\n",
       "       '101 Asian Buffet', '101 Bistro', '1130 The Restaurant', '12 East Cafe',\n",
       "       '12th Fairway Bar & Grill',\n",
       "       ...\n",
       "       'new siam', 'portale 50', 'reva zy', 'schuhs', 'sushi crystal',\n",
       "       'täglich', 'zpizza', 'À Table', 'Ém Cafe', 'Ô Deux Soeurs'],\n",
       "      dtype='object', name='name', length=7897)"
      ]
     },
     "execution_count": 45,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "poorly_reviewed.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorly_reviewed_rests = combined_data[combined_data.name.isin(poorly_reviewed.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419117"
      ]
     },
     "execution_count": 11,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(poorly_reviewed_rests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rest_corpus, bad_rest_dict = generate_corpus(poorly_reviewed_rests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_bus_corpus, good_bus_dict = generate_corpus(well_reviewed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['university', 'accepted']).trasnform(\n",
    "    lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[condition, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Good tutorial](https://radimrehurek.com/gensim/tut1.html#corpus-formats) on how to get Corpora in the right format.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nbviewer.ipython.org/gist/langmore/6820351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}