{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I use LDA to model latent topics. Latent models don't always yield useful results and it is difficult to know whether they will be useful or not until we actually start. \n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Group the Data by restauarant and then squish the review text together. \n",
    "\n",
    "Extensions:\n",
    "\n",
    "1. Consider only positive reviews. (Happy in the same say, upset for all different ways)\n",
    "2. COnsider only negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.ldamulticore import LdaModel\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_data = pd.read_hdf('../data/restaurant_reviews.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'review_id', 'text', 'user_id', 'city', 'latitude', 'longitude',\n",
       "       'name', 'neighborhoods', 'stars', 'hours'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_rest = combined_data.groupby(combined_data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15321"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_rest.stars.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = g_rest.text.apply(lambda x: '\\n'.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    So this place isn't spectacular, but it is def...\n",
       "1    I really like this place. The portions are lar...\n",
       "2    Delicious Sushi! Very big, fresh, tasty, flavo...\n",
       "3    Pizza was just plain, nothing special. I have ...\n",
       "4    I don't like the pizza hear and if you go for ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.text = text.text.apply(lambda x: gensim.utils.simple_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [so, this, place, isn, spectacular, but, it, i...\n",
       "1    [really, like, this, place, the, portions, are...\n",
       "2    [delicious, sushi, very, big, fresh, tasty, fl...\n",
       "3    [pizza, was, just, plain, nothing, special, ha...\n",
       "4    [don, like, the, pizza, hear, and, if, you, go...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import os\n",
    "# data_dir = '/Users/Will/anaconda/envs/py34/lib/python3.4/site-packages/spacy/en/data'\n",
    "# nlp = spacy.en.English(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make a gensim dictionary that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(242663 unique tokens: ['baghala', 'apes', 'luwan', 'soyons', 'souhaiter']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the Corpus into the format we want and save it to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start out with, we use the nltk stopwords. If this turns out to cause problems, we will also start excluding the most common words from the current corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_ids = list(map(dictionary.token2id.get, stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary.filter_tokens(stopwords_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?dictionary.filter_extremes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41555"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(t) for t in .text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('../data/deerwester.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.save('../data/words.dict') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should probably remove stopwords at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus \n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load('../data/words.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = MmCorpus('../data/deerwester.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend running this on a power EC2 server. Or else it will take too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_lda = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=20, workers=31,\n",
    "                       chunksize=50, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_lda.save('../data/ldamodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=242663, num_topics=20, decay=0.5, chunksize=100)\n"
     ]
    }
   ],
   "source": [
    "print(all_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.050*the + 0.033*and + 0.027*to + 0.019*it + 0.016*of + 0.014*was + 0.014*is + 0.013*for + 0.013*in + 0.011*this',\n",
       " '0.054*the + 0.037*and + 0.023*to + 0.019*pizza + 0.019*it + 0.019*was + 0.016*of + 0.015*is + 0.013*for + 0.012*in',\n",
       " '0.056*the + 0.039*and + 0.023*to + 0.019*was + 0.018*it + 0.017*is + 0.016*of + 0.013*for + 0.011*in + 0.010*food',\n",
       " '0.056*the + 0.032*and + 0.024*to + 0.020*was + 0.019*of + 0.017*it + 0.014*for + 0.013*is + 0.012*in + 0.011*buffet',\n",
       " '0.053*the + 0.036*and + 0.026*to + 0.017*of + 0.016*it + 0.016*was + 0.014*is + 0.012*for + 0.011*in + 0.010*we',\n",
       " '0.055*the + 0.037*and + 0.022*was + 0.022*to + 0.019*it + 0.015*of + 0.013*for + 0.012*we + 0.011*is + 0.010*in',\n",
       " '0.057*the + 0.037*and + 0.022*to + 0.019*it + 0.018*was + 0.017*of + 0.015*is + 0.013*in + 0.011*for + 0.010*pho',\n",
       " '0.056*the + 0.035*and + 0.023*to + 0.018*it + 0.018*was + 0.016*burger + 0.014*of + 0.012*is + 0.012*in + 0.011*for',\n",
       " '0.053*the + 0.035*and + 0.022*to + 0.019*it + 0.019*was + 0.016*is + 0.015*of + 0.012*in + 0.012*for + 0.012*food',\n",
       " '0.056*the + 0.038*and + 0.021*to + 0.018*was + 0.018*it + 0.015*of + 0.015*is + 0.012*in + 0.011*for + 0.010*food']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_single = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20,\n",
    "                     chunksize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.050*pad + 0.032*pittsburgh + 0.023*tom + 0.011*truck + 0.009*panang + 0.009*drunken + 0.007*ew + 0.006*satay + 0.006*papaya + 0.005*curries',\n",
       " '0.006*pepperoni + 0.004*subway + 0.004*zpizza + 0.004*philly + 0.003*factory + 0.003*refund + 0.003*pies + 0.002*hut + 0.002*cheesesteak + 0.002*wich',\n",
       " '0.021*brisket + 0.017*spaghetti + 0.012*ravioli + 0.010*lasagna + 0.009*ricotta + 0.009*bruschetta + 0.008*gelato + 0.008*marinara + 0.008*tiramisu + 0.007*pastas',\n",
       " '0.307*pho + 0.068*vietnamese + 0.033*unphogettable + 0.013*mi + 0.011*boba + 0.010*vermicelli + 0.010*sprouts + 0.010*brisket + 0.009*bo + 0.008*plaza',\n",
       " '0.013*charlotte + 0.006*grits + 0.004*cornbread + 0.004*tots + 0.003*vig + 0.003*uptown + 0.003*ahi + 0.003*soul + 0.003*kale + 0.002*lemonade',\n",
       " '0.034*pita + 0.031*hummus + 0.027*gyro + 0.018*falafel + 0.014*edinburgh + 0.013*feta + 0.012*mediterranean + 0.010*gyros + 0.007*kabob + 0.007*eastern',\n",
       " '0.019*pub + 0.013*madison + 0.010*shake + 0.008*pickles + 0.007*pretzel + 0.007*irish + 0.006*craft + 0.006*buns + 0.006*sliders + 0.006*curds',\n",
       " '0.045*asada + 0.043*carne + 0.024*burritos + 0.017*pastor + 0.015*tortillas + 0.015*torta + 0.012*el + 0.010*quesadilla + 0.009*pico + 0.009*viva',\n",
       " '0.017*noca + 0.013*foie + 0.008*truffle + 0.008*gras + 0.006*pairing + 0.005*risotto + 0.004*candy + 0.004*pairings + 0.004*mussels + 0.004*amuse',\n",
       " '0.005*yard + 0.004*wing + 0.004*band + 0.003*dance + 0.003*mill + 0.003*zipps + 0.003*upstairs + 0.002*ticket + 0.002*entertainment + 0.002*smoking']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_single.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_single.save('../data/all_lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Reviews Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "def generate_corpus(df, grouper='name', text='text'):\n",
    "    \"\"\"\n",
    "    Generates a dictionary with stopwords and most frequent words removed.\n",
    "    Also tells how many items have been removed.\n",
    "\n",
    "    :param df: pd.DataFrame() \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Combine the text on a restaurant level\n",
    "    grouped_df = df.groupby(grouper)\n",
    "    all_review_text = grouped_df[text].apply(lambda x: '\\n'.join(x)).reset_index()\n",
    "\n",
    "    # Do some Processing\n",
    "    all_review_text[text] = all_review_text[text].apply(\n",
    "        lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(all_review_text[text])\n",
    "    original_dict_length = len(dictionary)\n",
    "\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    stopwords_ids = list(map(dictionary.token2id.get, stopwords))\n",
    "    dictionary.filter_tokens(stopwords_ids)\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.1)\n",
    "\n",
    "    dictionary.compactify()\n",
    "    percent_removed = 1 - (len(dictionary) / original_dict_length)\n",
    "    logging.log('Removed {} items from our dictionary. {} %'\n",
    "                .format(original_dict_length - len(dictionary)),\n",
    "                percent_removed)\n",
    "\n",
    "    # Generate the corpus\n",
    "    corpus = [dictionary.doc2bow(t) for t in df[text]]\n",
    "\n",
    "    if SAVE:\n",
    "        gensim.corpora.MmCorpus.serialize('../data/deerwester.mm', corpus)\n",
    "        dictionary.save('../data/words.dict')\n",
    "        \n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_reviews = combined_data[combined_data.stars.isin([4,5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_corpus, good_dict = generate_corpus(good_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Reviews Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_reviews = combined_data[combined_data.stars.isin([1,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_corpus, bad_dict = generate_corpus(bad_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Good tutorial](https://radimrehurek.com/gensim/tut1.html#corpus-formats) on how to get Corpora in the right format.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nbviewer.ipython.org/gist/langmore/6820351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
